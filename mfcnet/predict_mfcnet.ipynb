{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import re\n",
    "from model.dmnet import DMNet\n",
    "from model.activations import swish\n",
    "from training.metrics import Metrics\n",
    "from training.trainer import Trainer\n",
    "from training.data_container import DataContainer\n",
    "from training.data_provider import DataProvider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/config_dmnet.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "F = config['F']\n",
    "L = config['L']\n",
    "K = config['K']\n",
    "r_cut = config['r_cut']\n",
    "atoms = config[\"atoms\"]\n",
    "num_interaction_blocks = config['num_interaction_blocks']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "target = config['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 14:10:21.576190: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "data_container = DataContainer(L, dataset, target, r_cut)\n",
    "\n",
    "data_provider = DataProvider(L, data_container, num_train, num_valid, batch_size, seed=data_seed, randomized=True)\n",
    "\n",
    "# Initialize datasets\n",
    "dataset = data_provider.get_dataset('test').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DMNet(F, L, K, r_cut, num_interaction_blocks, atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps, decay_steps, decay_rate, ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run date: 2024.02.06 12:33:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x107baf430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model from your own training run\n",
    "files = os.listdir(\"../logging\")\n",
    "rel_files = [f for f in files if model_name in f]\n",
    "rel_files.sort()\n",
    "#directory = \"logging/20230810_165359_densnet_Ael8lbyb_md_h2.npz_densities_final\"  # Fill this in\n",
    "# Get latest run\n",
    "directory = f\"../logging/{rel_files[-1]}\"\n",
    "run_date = re.search(r'(.{16})' + model_name, rel_files[-1]).group(1)\n",
    "run_date = run_date.replace(\"_\", \" \")\n",
    "run_date = run_date[:-1]\n",
    "run_date = run_date[:4] + \".\" + run_date[4:6] + \".\" + run_date[6:11] + \":\" + run_date[11:13] + \":\" + run_date[13:]\n",
    "print(f\"Run date: {run_date}\")\n",
    "best_ckpt_file = os.path.join(directory, 'best', 'ckpt')\n",
    "model.load_weights(best_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics('val', target)\n",
    "target_shape = data_provider.shape_target\n",
    "preds_total = np.zeros([36 * data_provider.nsamples['test']] + target_shape[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c2e7f262934c509f9517f79f47bc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps_per_epoch = int(np.floor(data_provider.nsamples['test'] / batch_size))\n",
    "preds_total = np.zeros([0] + target_shape[1:], dtype=np.float32)\n",
    "for step in tqdm(range(steps_per_epoch)):\n",
    "    preds, inputs = trainer.predict_on_batch(dataset_iter, metrics)\n",
    "    preds_total = np.concatenate([preds, preds_total], axis=0)\n",
    "    #preds_total[batch_start:batch_end] = preds.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.004576398059725761\n"
     ]
    }
   ],
   "source": [
    "preds_log_file = os.path.join(directory, 'preds.npz')\n",
    "log_dict = {}\n",
    "log_dict[\"MAE\"] = metrics.mean_mae\n",
    "log_dict[\"logMAE\"] = metrics.mean_log_mae\n",
    "log_dict[\"pred_densities\"] = preds_total\n",
    "log_dict[\"data_idx\"] = data_provider.idx['test']\n",
    "pickle.dump(log_dict, open(preds_log_file, \"wb\"))\n",
    "print(\"MAE:\", metrics.mean_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
